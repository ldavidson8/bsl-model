{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Config\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "NUM_CLASSES = 36 # 26 letters + 10 digits\n",
    "MODEL_PATH = \"bsl_model\"\n",
    "TFLITE_MODEL_PATH = \"bsl_model.tflite\"\n",
    "SEQUENCE_LENGTH = 30 # Number of frames to consider for each sample\n",
    "\n",
    "# Extract hand landmarks from video\n",
    "def extract_landmarks_from_video(video_path, max_frames=SEQUENCE_LENGTH):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    landmarks_sequence = []\n",
    "\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,  # BSL often uses two hands\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    ) as hands:\n",
    "        frame_count = 0\n",
    "        while cap.isOpened() and frame_count < max_frames:\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process image with MediaPipe\n",
    "            results = hands.process(image_rgb)\n",
    "\n",
    "            # Extract landmarks if hands are detected\n",
    "            frame_landmarks = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Get handedness (left or right)\n",
    "                    handedness = (\n",
    "                        \"Left\"\n",
    "                        if results.multi_handedness[0].classification[0].label == \"Left\"\n",
    "                        else \"Right\"\n",
    "                    )\n",
    "\n",
    "                    # Extract landmarks\n",
    "                    landmarks = []\n",
    "                    for point in hand_landmarks.landmark:\n",
    "                        landmarks.extend([point.x, point.y, point.z])\n",
    "\n",
    "                    # Add handedness as a feature (0 for left, 1 for right)\n",
    "                    handedness_feature = 1.0 if handedness == \"Right\" else 0.0\n",
    "                    landmarks.append(handedness_feature)\n",
    "\n",
    "                    frame_landmarks.append(landmarks)\n",
    "\n",
    "            # If no hands detected, add zeros\n",
    "            if not frame_landmarks:\n",
    "                # 21 landmarks Ã— 3 coordinates + 1 handedness feature = 64 values\n",
    "                frame_landmarks = [[0.0] * 64]\n",
    "\n",
    "            # Add to sequence\n",
    "            landmarks_sequence.append(frame_landmarks)\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Pad sequence if needed\n",
    "    while len(landmarks_sequence) < max_frames:\n",
    "        landmarks_sequence.append([[0.0] * 64])\n",
    "\n",
    "    # Truncate if too long\n",
    "    landmarks_sequence = landmarks_sequence[:max_frames]\n",
    "\n",
    "    return landmarks_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset videos to extract landmarks\n",
    "def process_dataset(dataset_dir):\n",
    "    dataset = {\n",
    "        \"landmarks\": [],\n",
    "        \"labels\": [],\n",
    "        \"handedness\": [],  # 'left', 'right', or 'both'\n",
    "        \"num_hands\": [],  # 1 or 2\n",
    "    }\n",
    "\n",
    "    # Walk through dataset directory\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "                # Extract class from path\n",
    "                path_parts = root.split(os.sep)\n",
    "                # Assuming structure like .../training/letters/A/right_handed/one_handed/\n",
    "                sign_class = None\n",
    "                handedness = None\n",
    "                num_hands = None\n",
    "\n",
    "                # Find relevant parts in path\n",
    "                for i, part in enumerate(path_parts):\n",
    "                    if part in (\"letters\", \"numbers\"):\n",
    "                        if i + 1 < len(path_parts):\n",
    "                            sign_class = path_parts[i + 1]\n",
    "                    if part in (\"right_handed\", \"left_handed\"):\n",
    "                        handedness = \"right\" if part == \"right_handed\" else \"left\"\n",
    "                    if part in (\"one_handed\", \"two_handed\"):\n",
    "                        num_hands = 1 if part == \"one_handed\" else 2\n",
    "\n",
    "                if sign_class and handedness and num_hands:\n",
    "                    video_path = os.path.join(root, file)\n",
    "                    landmarks_sequence = extract_landmarks_from_video(video_path)\n",
    "\n",
    "                    dataset[\"landmarks\"].append(landmarks_sequence)\n",
    "                    dataset[\"labels\"].append(sign_class)\n",
    "                    dataset[\"handedness\"].append(handedness)\n",
    "                    dataset[\"num_hands\"].append(num_hands)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "def prepare_data(dataset):\n",
    "    # Convert labels to numerical\n",
    "    unique_labels = sorted(set(dataset[\"labels\"]))\n",
    "    label_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Save label mapping\n",
    "    with open(\"label_mapping.json\", \"w\") as f:\n",
    "        json.dump(label_to_idx, f)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(dataset[\"landmarks\"])\n",
    "    y = np.array([label_to_idx[label] for label in dataset[\"labels\"]])\n",
    "\n",
    "    # Reshape X to (num_samples, sequence_length, num_features)\n",
    "    # Each frame can have up to 2 hands with 64 features each, so max 128 features per frame\n",
    "    X_reshaped = np.zeros((len(X), SEQUENCE_LENGTH, 128))\n",
    "\n",
    "    for i, seq in enumerate(X):\n",
    "        for j, frame in enumerate(seq):\n",
    "            if len(frame) == 1:  # One hand detected\n",
    "                X_reshaped[i, j, :64] = frame[0]\n",
    "            elif len(frame) == 2:  # Two hands detected\n",
    "                X_reshaped[i, j, :64] = frame[0]\n",
    "                X_reshaped[i, j, 64:] = frame[1]\n",
    "\n",
    "    # One-hot encode labels\n",
    "    y_one_hot = tf.keras.utils.to_categorical(y, num_classes=len(unique_labels))\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_reshaped, y_one_hot, test_size=0.3, random_state=42, stratify=y_one_hot\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, label_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model for sequence data\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            # LSTM layers for sequence processing\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(64, return_sequences=True), input_shape=input_shape\n",
    "            ),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Bidirectional(layers.LSTM(32)),\n",
    "            layers.Dropout(0.3),\n",
    "            # Dense layers for classification\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=EPOCHS):\n",
    "    # Create model directory\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "    # Add callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                MODEL_PATH, \"model-{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "            ),\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-6),\n",
    "    ]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Lite\n",
    "def convert_to_tflite(model, tflite_path):\n",
    "    # Convert to TensorFlow Lite model\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "    # Enable optimizations\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Convert the model\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the TFLite model\n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"TensorFlow Lite model saved to {tflite_path}\")\n",
    "\n",
    "    # Get model size info\n",
    "    tflite_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "    print(f\"TensorFlow Lite Model size: {tflite_size:.2f} MB\")\n",
    "\n",
    "    return tflite_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single frame for inference\n",
    "def process_frame_for_inference(frame, hands_model):\n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process image with MediaPipe\n",
    "    results = hands_model.process(image_rgb)\n",
    "\n",
    "    # Extract landmarks if hands are detected\n",
    "    frame_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get handedness\n",
    "            handedness = (\n",
    "                \"Left\"\n",
    "                if results.multi_handedness[0].classification[0].label == \"Left\"\n",
    "                else \"Right\"\n",
    "            )\n",
    "\n",
    "            # Extract landmarks\n",
    "            landmarks = []\n",
    "            for point in hand_landmarks.landmark:\n",
    "                landmarks.extend([point.x, point.y, point.z])\n",
    "\n",
    "            # Add handedness as a feature (0 for left, 1 for right)\n",
    "            handedness_feature = 1.0 if handedness == \"Right\" else 0.0\n",
    "            landmarks.append(handedness_feature)\n",
    "\n",
    "            frame_landmarks.append(landmarks)\n",
    "\n",
    "    # If no hands detected, add zeros\n",
    "    if not frame_landmarks:\n",
    "        frame_landmarks = [[0.0] * 64]\n",
    "\n",
    "    # Reshape for model input (assuming model expects sequence of frames)\n",
    "    features = np.zeros((1, 128))\n",
    "\n",
    "    if len(frame_landmarks) == 1:  # One hand detected\n",
    "        features[0, :64] = frame_landmarks[0]\n",
    "    elif len(frame_landmarks) == 2:  # Two hands detected\n",
    "        features[0, :64] = frame_landmarks[0]\n",
    "        features[0, 64:] = frame_landmarks[1]\n",
    "\n",
    "    return features, results.multi_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing information\n",
    "def save_preprocessing_info(label_to_idx):\n",
    "    preprocessing_info = {\n",
    "        \"label_to_idx\": label_to_idx,\n",
    "        \"sequence_length\": SEQUENCE_LENGTH,\n",
    "    }\n",
    "\n",
    "    with open(\"preprocessing_info.pkl\", \"wb\") as f:\n",
    "        pickle.dump(preprocessing_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main(dataset_dir):\n",
    "    print(\"Processing dataset...\")\n",
    "    dataset = process_dataset(dataset_dir)\n",
    "\n",
    "    print(f\"Dataset processed. Total samples: {len(dataset['labels'])}\")\n",
    "\n",
    "    print(\"Preparing data for training...\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_to_idx = prepare_data(dataset)\n",
    "\n",
    "    print(f\"Data prepared. Training set size: {X_train.shape}\")\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model(\n",
    "        input_shape=(SEQUENCE_LENGTH, 128), num_classes=len(label_to_idx)\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save(os.path.join(MODEL_PATH, \"bsl_final_model.h5\"))\n",
    "\n",
    "    print(\"Converting to TensorFlow Lite...\")\n",
    "    tflite_path = convert_to_tflite(model, TFLITE_MODEL_PATH)\n",
    "\n",
    "    print(\"Saving preprocessing information...\")\n",
    "    save_preprocessing_info(label_to_idx)\n",
    "\n",
    "    return tflite_path, label_to_idx\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_dir = \"bsl_dataset\"  # Change to your dataset directory\n",
    "    tflite_path, label_to_idx = main(dataset_dir)\n",
    "    print(\n",
    "        f\"Model training and conversion complete. TFLite model saved to: {tflite_path}\"\n",
    "    )\n",
    "    print(f\"Label mapping: {label_to_idx}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
